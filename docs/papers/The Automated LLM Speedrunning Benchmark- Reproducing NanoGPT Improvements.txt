arXiv:2506.22419v2  [cs.AI]  30 Jun 2025
The Automated LLM Speedrunning Benchmark:
Reproducing NanoGPT Improvements
Bingchen Zhao∗,1,2,Despoina Magka∗,1,Minqi Jiang∗,1
Xian Li1,Roberta Raileanu1,Tatiana Shavrina1,Jean-Christophe Gagnon-Audet1,Kelvin Niu1,Shagun
Sodhani1,Michael Shvartsman1,Andrei Lupu1,Alisia Lupidi1,Edan Toledo1,Karen Hambardzumyan1,
Martin Josifoski1,Thomas Foster1,Lucia Cipolina-Kun1,Abhishek Charnalia1,Derek Dunfield1,
Alexander H. Miller1,Oisin Mac Aodha2,Jakob Foerster1,Yoram Bachrach1
1Meta,2University of Edinburgh
∗Equal contribution
Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress.
A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the
ability of AI agents to reproduce results in an active research area, we introduce the Automated
LLM Speedrunning Benchmark, leveraging the research community’s contributions on the NanoGPT
speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks
provides the agent with the previous record’s training script, optionally paired with one of three hint
formats, ranging from pseudocode to paper-like descriptions of the new record’s improvements. Records
execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging
from high-level algorithmic advancements to hardware-aware optimizations. These features make the
benchmark both accessible and realistic for the frontier problem of improving LLM training. We find
that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known
innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple,
non-saturated measure of an LLM’s ability to automate scientific reproduction, a necessary (but not
sufficient) skill for an autonomous research agent.
Date:July 2, 2025
Code:https://github.com/facebookresearch/llm-speedrunner
1 Introduction
0 20 40 60 80 100
Percentage of Speedup RecoveredNo Hints Pseudocodeo3-mini
Claude 3.7 Sonnet
Gemini-2.5-Pro
DeepSeek-R1
Figure 1 Recent LLM agents struggle to reproduce
NanoGPT Speedrun records.The advent of LLMs capable of succeeding in challenging
math, coding, and scientific reasoning domains has led
to a surge of activity in applying LLM agents to the
longstanding ambition of automated scientific discov-
ery (Simon, 1995; Langley, 1987; Waltz and Buchanan,
2009; King et al., 2009; Steinruecken et al., 2019). Early
results suggest LLM-based systems can improve the
productivity of human researchers, from formulating
hypotheses to implementing code-based experiments to
testing them (Romera-Paredes et al., 2024; Castro et al.,
2025; Yin, 2025; Inizan et al., 2025).
Scientific progress hinges on trustworthy results, and the
ultimate test of the truth behind a finding is whether
the experiment and its outcomes can be reproduced (Fineberg et al., 2019; Pineau et al., 2021; Henderson
et al., 2018). Thus, a critical component of automated science is automated reproducibility : the process of
automatically reimplementing an experiment based on a description of the experiment design, such that the
implementation reproduces previously reported outcomes. In other words, translating the description of an
experiment into its implementation (Peng, 2011; Siegel et al., 2024). Moreover, success in reimplementing a
1Original NanoGPT 
tr ain_gpt2.p yR ecor d 2DescriptionsOfficial change logPseudocode (le v el 1)T e xt description (le v el 2)Mini-paper (le v el 3)DescriptionsOfficial change logPseudocode (le v el 1)T e xt description (le v el 2)Mini-paper (le v el 3)R ecor d 1DescriptionsOfficial change logPseudocode (le v el 1)T e xt description (le v el 2)Mini-paper (le v el 3)R ecor d 3DescriptionsOfficial change logPseudocode (le v el 1)T e xt description (le v el 2)Mini-paper (le v el 3)DescriptionsOfficial change logPseudocode (le v el 1)T e xt description (le v el 2)Mini-paper (le v el 3)DescriptionsOfficial change logPseudocode (le v el 1)T e xt description (le v el 2)Mini-paper (le v el 3)Human
Exper tHuman
Exper tHuman
Exper t
A gentA gentSimilarityFSR2FSR = ΣFSRi / # r ecor dsT otal F r action of
Speedup R eco v er edSimilarityFSR1.........Code,
Code,Code,Code,Figure 2 The Automated LLM Speedrunning Benchmark. We create a task for each consecutive pair of records Ri,
Ri+1. The performance of the agent is evaluated by comparing the relative speedup of the agent solution R′
itoRi.
known study also serves as a metric for assessing the reliability with which an agent can implement experiments
via description, an ability that would enable researchers to quickly scale up the testing of new ideas, regardless
of whether they are of human or AI origin.
We study the ability of recent reasoning LLMs in combination with state-of-the-art scaffolds—programs that
iteratively make use of an LLM for finding a solution to a given task—on reproducing prior discoveries in
the domain of LLM training. We henceforth refer to the combination of a specific LLM and scaffold for the
purpose of automated research as a research agent , and use the more specific term AI research agent to refer
to those specifically designed for automating AI research itself. While there is much speculation that AI
research agents may lead to the beginnings of a recursive self-improvement loop for future LLM-based research
agents, we set our focus on the more modest goal of understanding whether current AI research agents can
succeed at the prerequisite task of reproducing previous scientific findings on GPT-2 (Radford et al., 2019),
the first model to demonstrate a broad capacity for zero-shot transfer to new tasks via prompting.
Towards this goal, we introduce The Automated LLM Speedrunning Benchmark , based on the series of
community-driven improvements to GPT-2 training in the NanoGPT Speedrun (Jordan et al., 2024a), a
competition based on minimizing the wall time of training an open-source PyTorch reimplementation of
GPT-2 (Karpathy, 2023) to reach a target cross-entropy loss of 3.28 on the validation set of FineWeb (Penedo
et al., 2024), using a single 8 ×H100 node. Since its inception in June 2024, this community effort has driven
the training time of GPT-2 from 45 minutes to below 3 minutes (as of May 2025). These improvements were
driven by new algorithmic enhancements, some of which have been shown to generalize beyond the scale of
the 124M parameter GPT-2 model, with the most notable being the invention of the Muon optimizer (Jordan
et al., 2024b), later demonstrated to show benefits for training much larger modern LLMs (Liu et al., 2025a;
Shah et al., 2025). Other speedrun improvements include mixed precision training and more efficient attention
variants (Dong et al., 2024). As of May 2025, the NanoGPT Speedrun includes 21 successive speedrun records.
Each record is associated with its corresponding training script ( train_gpt.py ), a measured training time, a
public announcement of the changes, and a high-level summary of the code changes.1
The Automated LLM Speedrunning Benchmark then tasks an AI research agent with reproducing each
successive speedrun record, starting from the previous record, with an optional set of hints of various formats
and levels of detail. The clear code-level ground-truth targets per record alongside detailed change logs
between records make this benchmark an ideal testing ground for the ability of agents to reproduce not
only a single experimental finding, but also a series of cumulative research findings—a distinct affordance
compared to prior reproducibility benchmarks. Here, all tasks share the same success metric of training time
to reach the target validation loss, measured on a fixed hardware configuration (a single 8xH100 node), making
exact reproduction, fair comparisons, and cross-task comparisons straightforward. Lastly, perhaps the most
compelling aspect of this benchmark is its focus on reproducing discoveries directly relevant to real-world
LLM development.
1https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#world-record-history
2Table 1Key motivations of our benchmark design and how it differentiates from existing ML reproducibility benchmarks.
Here, “Reproducibility” denotes whether the tasks require replicating a given technique; “Sequential”, whether the
benchmark measures reproducibility over a cumulative series of scientific results; “LLM research”, whether the task
involves language model development; and “Agent scaffold”, whether a baseline agent scaffold is released with the
benchmark.
Reproducibility Sequential LLM research Agent scaffold
MLE-bench (Chan et al., 2025) No No No No
PaperBench (Starace et al., 2025) Yes No Partially Yes
CORE-bench (Siegel et al., 2024) Yes No No Yes
RE-bench (Wijk et al., 2024) No No Yes Yes
MLAgentBench (Huang et al., 2024) No No Partially Yes
MLGym-bench (Nathani et al., 2025) No No Partially Yes
Automated LLM Speedrunning (ours) Yes Yes Yes Yes
Our experiments show that even when given a description of the difference between two consecutive speedrun
recordsinvariousformats, recentagentsbasedonDeepSeek-R1(DeepSeek-AIetal.,2025)ando3-mini(OpenAI,
2025) combined with a state-of-the-art search scaffold, still struggle to improve ground-truth records to match
the speedups of the next ground-truth record (see Figure 1).
We believe the Automated LLM Speedrunning Benchmark can spur development of AI research agents that
can automate reproducibility studies, paving a critical step on the way towards more capable AI research
agents that can realize the aspiration of accelerating the pace of scientific discovery via automated science.
However, our results show that before such lofty goals can be realized, automated reproducibility remains a
central challenge that must be addressed.
2 Related works
Automated reproducibility. Recent works have devised benchmarks for evaluating the ability of LLM agents
to reproduce code-based experiments from published papers. CORE-Bench measures an agent’s ability to
correctly install, execute, and interpret a paper’s associated codebase and its outputs (Siegel et al., 2024).
Other benchmarks, including PaperBench (Starace et al., 2025), Papers2Code (Seo et al., 2025), AutoP2C (Lin
et al., 2025), and SciReplicate (Xiang et al., 2025) test the agent’s ability to convert a research paper to a
codebase that replicates the reported findings or the agent’s ability to formulate and test hypotheses (Chen
et al., 2025; Liu et al., 2025b). Instead of evaluating on a wide set of, often, unrelated papers as in these
previous works, the Automated LLM Speedrunning Benchmark focuses on a single important overarching
task of speeding up LLM training. This focus allows for a unified success metric across a diverse gradation of
task complexity, defined by the natural path of innovation previously discovered by human researchers. This
grounding allows for not only comparison to granular, ground-truth code-level changes, but also opens the
door to evaluating an LLM agent’s ability to reproduce an entire research arc over multiple compounding
innovations against human performance. Moreover, the benchmark’s multiple hint levels allow for controlled
study of how performance varies across different forms of background information.
Code generation with LLMs. Code is inherently reproducible via repeated execution and requires no additional
equipment to run beyond a computer. Thus, many automated scientific reproducibility benchmarks, including
ours, focus primarily on virtual, code-based experiments. In this domain, research agents directly benefit
from and build upon the rapid progress in coding and computer-use agents, such as a growing set of complex,
sandboxed software-engineering agent benchmarks (Yang et al., 2024; Wang et al., 2024; Fourney et al., 2024;
Mialon et al., 2023; Yoran et al., 2024; Zhou et al., 2023; Koh et al., 2024) and scaffold designs (Zhang et al.,
2024), such as AIDE (Jiang et al., 2025), which we both use as a baseline and extend in our experiments.
LLMs for automated ML. Recent advances enabling LLMs to exploit chain-of-thought outputs during inference
have led to drastic improvements in their performance on reasoning tasks in domains like math, coding, and
science. These improvements have led to a surge in LLM programs seeking to automate the key parts of
machine learning itself, encompassing iterated hypothesis generation and testing and the writing of reports
3detailing the findings, in the form of end-to-end agents (Lu et al., 2024; Huang et al., 2025; Yamada et al.,
2025a), agents focused on hypothesis generation (Gottweis et al., 2025; O’Neill et al., 2025), as well as
agents that can interact with a human-in-the-loop to jointly formulate and test hypotheses (Intology AI,
2025; Autoscience Institute, 2025). However, early results suggest these systems, while capable of optimizing
code-level improvements, often fall short in executing on experiments that faithfully reflect their intended
goals (Yamada et al., 2025b). Thus, while LLM-based reasoning models can generate, at times, novel
hypotheses (Gu et al., 2024), their ability for scientific reproduction remains a crucial bottleneck in automating
scientific research.
3 The Automated LLM Speedrunning Benchmark
The Automated LLM Speedrunning Benchmark seeks to evaluate an LLM agent’s ability to reproduce the
wall-time speedup associated with each record transition from the NanoGPT Speedrun, both with and without
access to hints describing the corresponding changes at varying levels of abstraction. Table 1 summarizes how
our work compares to existing ML reproducibility benchmarks.
3.1 Reproducibility tasks from existing records
For each transition from record Ri−1to record Rifori= 2, ...,21, excluding i= 7, whose speedup is purely
due to upgrading PyTorch, we define the following components:
RiTraining script for the i-th record in the speedrun ,
tiWall-clock time (in seconds) required by Rito reach the target validation loss ,
∆1
iLevel 1 hint: A pseudocode description of code change from the previous record ,
∆2
iLevel 2 hint: A natural-language description of the code change from the previous record ,
∆3
iLevel 3 hint: A mini-paper summarizing the code change from the previous record .
All hints were first drafted by R1, manually verified, and, where necessary, edited for correctness and relevance.
See Appendix D for further details on our hint creation process. We provide a categorized listing of all
ground-truth records in Appendix E and example hints in Appendix F.
For convenience, we denote the set of ground-truth speedrun records (which excludes record 6) as I. We define
arecord task as a tuple ⟨Ri−1,Ri, ti, m⟩, where R1corresponds to the initial NanoGPT training script, and
where mis any subset of the set of hint levels ,{0,1,2,3}, where level 0 corresponds to no hint. Depending on
the presence of hints, we categorize the possible tasks in our benchmark into two types:
Record reproduction tasks. Given hints that describe the subsequent record, i.e. m̸={0}, the LLM agent
must reproduce record Ri+1givenRiand the set of corresponding hints. Here the key metric of interest is
thefraction of speedup recovered (FSR), defined as
FSR i=ti−t′
i+1
ti−ti+1. (1)
where t′
i+1is the training time achieved by the agent to reach the target validation loss. The full benchmark
performance is then the mean FSR over the set of all included records, I:
FSR =1
|I|X
iti−t′
i+1
ti−ti+1. (2)
Record optimization tasks. Without any hints, i.e. m={0}, the LLM agent must produce a new training
script solution R′
i+1with a minimal training time t′
i+1to reach the target validation loss, given Ri. Here we
consider both the raw wall time t′
i+1of the solution produced, in addition to FSR i. Similar to the setting of
record reproduction, we consider the mean of these metrics over all ground-truth records in the benchmark as
an overall measure of performance. This allows the agent to explore its own improvements given the same
SoTA starting point that humans had when each record was produced.
4codere x ecut oranalyz erv*Select
ne xt 
v ersion
r esult s
t emplat er esult s.jsonsolution.p yv0v1v2......Initializ e r oot node
cop y t emplat e int o r oot v ersionDraft
cr eat e N0 initial solutionsImpr o v e
pr oduce N impr o v ed solutions fr om par ent s
based on some selection crit erionDebug
debug  v ersion wit h pr obability pdebugbuggy
kno wledgepaper .mdf act s.md
vN0vN0+1vN0+1vN0+NvN0+N+1Figure 3 Overview of our flexible search scaffold. Search starts from a root node containing code for the starting
record Rifrom which N0initial solutions are generated. Subsequently, each search iteration debugs a buggy leaf node
with probability pdebugand otherwise greedily selects the best node to improve, with debug and improvement each
branching Nsolutions. At each search step, the coder submodule implements the solution, with optional access to
external knowledge (e.g. hints).
3.2 Agent scaffolds
We provide a flexible search scaffold implementation that extends AIDE (Jiang et al., 2025) into a more general
parameterization. In this setup, visualized in Figure 3, each node in the search tree represents a solution
instance contained in a directory with relevant scripts, performance metrics, and an LLM-generated execution
summary. For instance, in NanoGPT training, a solution node consists of a single train_gpt2.py script and a
results file describing its performance and execution outcome. The fitness of each node is evaluated based on
these metrics—such as wall time to reach the target validation loss—with each new search initialized using a
ground-truth script from the benchmark and proceeding by branching into up-to-multiple child solutions.
Each search step follows three stages: implementation, execution, and analysis. During implementation,
the agent generates working code from a prompt that includes the task description and optionally, a set of
associated hints. We use Aider (Gauthier, 2025) to make diff-based edits to the initial solution, producing
modified versions for execution. These solutions are then run on an 8xH100 node, and the output is summarized
in natural language via the analysis stage, capturing key performance indicators and insights from standard
outputs. Custom prompts guide each stage and are detailed in Appendix D. The search begins with N0initial
modifications to the root node. At each step, a new node branches from either a randomly chosen buggy
node (with probability pdebug) or the highest-performing node. To avoid redundant debugging, we cap retries
atDmaxper node. This scaffold design supports multiple search variants, outlined in Table 2, with each
receiving the same budget Mof search steps to ensure fair comparison.
Table 2Search variants and their corresponding scaffold parameterizations.
Method Initial branch factor Branch factor Debug probability Max debug depth
Tree 1 N 0 0
Forest N0 N 0 0
AIDE N0 1 pdebug Dmax
Multi-AIDE N0 N p debug Dmax
Flat (Best-of-M) M — — —
501231+21+2+30.00.20.40.60.8FSRFlat
01231+21+2+3Tree
01231+21+2+3Forest
01231+21+2+3AIDE
01231+21+2+3Multi-AIDEDeepSeek-R1 o3-mini Gemini-2.5-Pro Claude 3.7 SonnetFigure 4 Mean FSR across five search variants and four frontier models for six hint regimes: no hint ( 0), pseudocode
(1), text ( 2), mini-paper ( 3) and combinations thereof ( 1 + 2,1 + 2 + 3 ).
0.00 0.05 0.10 0.15 0.20
FSR01231 + 21 + 2 + 3Hint Level
0.00 0.05 0.10 0.15 0.20
FSRFlatTreeForestAIDEMulti-AIDEScaffold
0.00 0.05 0.10 0.15 0.20
FSRDeepSeek-R1o3-miniGemini-2.5-ProClaude-3.7-SonnetModel
Figure 5 Interquartile Mean (IQM) evaluation results. Scores are aggregated across multiple runs with the same hint
level, scaffold, and model.
4 Experiments and results
We now evaluate the performance of several baseline agents across a range of scaffolds, hint formats, and model
backbones for all NanoGPT Speedrun records. We report results using the normalized runtime improvement
metric (FSR) from Equation 2, as well as measures of code similarity between agent and human solutions.
For fair comparisons, we use training times for human records based on rerunning each ground-truth record
on the same hardware configuration as agent solutions. Appendix A reports the near exact reproduction of
training times for human records on our cluster.
4.1 Baselines
We compare a number of LLM agents based on DeepSeek-R1, o3-mini, Gemini-2.5-Pro, and Claude-3.7-Sonnet,
using instances of the search scaffolds listed in Table 2. Our choice of parameters are N0= 3for the initial
pool of root hypotheses (forest, AIDE and multi-AIDE), N= 3for the branching factor (tree, forest and
multi-AIDE), pdebug = 0.5andDmax= 5for the debug probability and maximum debug depth respectively
(AIDE and multi-AIDE), and a search budget of M= 20nodes. Taken together, these scaffolds cover a range
of branching factors, search depth, and debug logic.
For each pair of model and search scaffold, we assess the mean FSR across all 19 tasks for each of the following
hint levels: no hint (level 0), pseudocode (level 1), text description (level 2), and mini-paper (level 3). Each
solution is executed under a maximum runtime of 60 minutes (i.e. a maximum of 20 hours per agent run).
We observe an average run time of ≈10 hours per agent run, across a total of 6,840 agent runs (19 records ×
6 hint regimes ×5 search variants ×4 models ×3 seeds), for a total of 6,840 ×8 H100 (internal cluster)
hours spent executing the generated solutions.
4.2 Reproducing individual records
We report the mean FSR for each model, search scaffold, and hint-level combination across 3 full search runs
in Figure 4, including the case of no hints. It is evident that hints are necessary for inducing greater values
of FSR, with all agents failing to recover more than 20% of the speed-up achieved by human solutions on
average without hints. Appendix B further reports the mean FSR for each individual record transitions per
agent variation across 3 runs per variation.
60.00.20.40.60.81.01.21.4FSRR²=0.00
R²=0.00
R²=0.00
R²=0.03Level 0
R²=0.06
R²=0.07
R²=0.07
R²=0.05Level 1
R²=0.00
R²=0.07
R²=0.07
R²=0.06Level 2
0.4
 0.2
 0.0 0.2 0.4
L2 distance recovered0.00.20.40.60.81.01.21.4FSRR²=0.17
R²=0.16
R²=0.08
R²=0.08Level 3
0.4
 0.2
 0.0 0.2 0.4
L2 distance recoveredR²=0.03
R²=0.07
R²=0.05
R²=0.23Level 1+2
0.4
 0.2
 0.0 0.2 0.4
L2 distance recoveredR²=0.27
R²=0.18
R²=0.01
R²=0.00Level 1+2+3DeepSeek-R1 o3-mini Gemini-2.5-Pro Claude 3.7 SonnetFigure 7 Correlation of FSR with L2 distance recovered for each hint level, showing a modest correlation between
similarity to the human solution and FSR for most hint types and models.
123457891011121314151617181920
Starting Record Index0.00.20.40.60.8FSR
0.000.050.100.150.200.25
L2 Distance Recovered
Figure 6 FSR and embedding distance per record for
o3-mini with text description hints (mean and std
over 3 seeds). Later records tend to be harder for
agents, leading to lower recovered embedding distance
and speedups.We observe that o3-mini generally achieves equal or
better results than other models in mean FSR for all
hint levels, but sees slightly worse performance with no
hints. Notably, flat search (i.e. best-of-M), generally
matches or outperforms iterated search scaffolds across
the individual hint levels (levels 1–3), while matching
their performance in the case of no hints. Moreover, tree
and forest methods, which lack debug steps, perform on
par with AIDE-based search scaffolds, suggesting that
explicit debug steps do not provide a significant benefit
on top of iterative improvement steps. Overall, the
gap between the best models (o3-mini and Claude-3.7-
Sonnet)andtheopen-weights(R1)iswiderforthesearch
scaffolds incorporating branching logic (tree, search, and
AIDE variants), suggesting that models like o3-mini
can better iterate on their previous solutions. Figure 6
further shows how agents tend to have more difficulty in
reproducing later records.
Out of the various hint formats, the most useful are the pseudocode and the combinations of pseudocode
with text and mini-papers hints, which enable o3-mini to recover approximately 40% and 46%, respectively,
of the speed-up attained by human solutions on average. Surprisingly, R1 agents seem to worsen with the
presence of the individual hints, generally achieving lower FSR compared to the no-hint setting, suggesting
that attempting to implement the complex changes in these hints results in buggy code. With hints, R1
produces solutions with lower FSR than simply making no changes to the code, a common outcome with no
hints, as indicated by the cluster around a recovered L2 embedding distance of 0.0 in Figure 7 (Section 4.6
details this similarity analysis).
7Table 3Performance comparison across different hint formats (mean and std over 3 runs). Color-coded values are
differences relative to the best-performing individual hint in the combination.
Hints Model Flat Tree Forest AIDE Multi-AIDE
L1 (pseudocode) o3-mini 0.40±0.02 0.43±0.02 0.40±0.02 0.41±0.02 0.43 ±0.02
L2 (text) o3-mini 0.22 ±0.04 0.16 ±0.03 0.26 ±0.04 0.18 ±0.02 0.17 ±0.03
L3 (mini-paper) o3-mini 0.17 ±0.03 0.13 ±0.03 0.15 ±0.04 0.12 ±0.01 0.25 ±0.04
L1+L2 o3-mini 0.27 ±0.03 (-0.13) 0.38±0.02 (-0.05) 0.31±0.04 (-0.09) 0.34±0.03 (-0.07) 0.37±0.03 (-0.06)
L1+L2+L3 o3-mini 0.24 ±0.05 (-0.16) 0.35±0.05 (-0.08) 0.39±0.03 (-0.01) 0.36±0.04 (-0.05) 0.46±0.04 (+0.03)
L1 (pseudocode) DeepSeek-R1 0.13 ±0.03 0.20 ±0.00 0.07 ±0.00 0.09 ±0.02 0.16 ±0.01
L2 (text) DeepSeek-R1 0.10 ±0.01 0.07 ±0.00 0.06 ±0.00 0.06 ±0.01 0.07 ±0.00
L3 (mini-paper) DeepSeek-R1 0.13 ±0.04 0.10 ±0.03 0.09 ±0.03 0.14 ±0.02 0.20 ±0.03
L1+L2 DeepSeek-R1 0.25 ±0.01 (+0.12) 0.20±0.03 (+0.00) 0.25±0.03 (+0.18) 0.28±0.03 (+0.19) 0.24±0.02 (+0.08)
L1+L2+L3 DeepSeek-R1 0.30±0.04 (+0.17) 0.24±0.02 (+0.04) 0.40±0.04 (+0.31) 0.36±0.03 (+0.22) 0.41±0.02 (+0.21)
L1 (pseudocode) Gemini-2.5-Pro 0.18 ±0.02 0.16 ±0.02 0.23 ±0.04 0.13 ±0.02 0.23 ±0.03
L2 (text) Gemini-2.5-Pro 0.18 ±0.01 0.18 ±0.03 0.19 ±0.02 0.09 ±0.01 0.16 ±0.03
L3 (mini-paper) Gemini-2.5-Pro 0.18 ±0.04 0.18±0.02 0.24 ±0.02 0.15 ±0.02 0.16 ±0.03
L1+L2 Gemini-2.5-Pro 0.18 ±0.02 (+0.00) 0.12±0.03 (-0.06) 0.24±0.04 (+0.01) 0.20±0.04 (+0.07) 0.19±0.04 (-0.04)
L1+L2+L3 Gemini-2.5-Pro 0.19±0.04 (+0.01) 0.14±0.04 (-0.04) 0.25±0.04 (+0.01) 0.17±0.03 (+0.02) 0.26±0.05 (+0.03)
L1 (pseudocode) Claude-3.7-Sonnet 0.14 ±0.03 0.13 ±0.03 0.05 ±0.01 0.14 ±0.01 0.18 ±0.04
L2 (text) Claude-3.7-Sonnet 0.10 ±0.03 0.03 ±0.01 0.06 ±0.02 0.14 ±0.02 0.14 ±0.02
L3 (mini-paper) Claude-3.7-Sonnet 0.06 ±0.02 0.22 ±0.02 0.11 ±0.01 0.34±0.01 0.19 ±0.03
L1+L2 Claude-3.7-Sonnet 0.14 ±0.03 (+0.00) 0.11±0.02 (-0.11) 0.15±0.02 (+0.04) 0.30±0.02 (-0.04) 0.09±0.01 (-0.09)
L1+L2+L3 Claude-3.7-Sonnet 0.21±0.04 (+0.07) 0.31±0.02 (+0.09) 0.10±0.02 (-0.01) 0.31±0.01 (-0.03) 0.20±0.02 (+0.01)
4.3 Combining multiple hints
We further investigate the impact of combining hint formats, and also include these results for each agent
variation in Figure 4. We observe that providing the text description or mini-paper together with the
pseudocode compared to only providing the pseudocode hint can substantially degrade performance for
o3-mini (see o3-mini result in Table 3), but surprisingly benefits R1. These results suggest that o3-mini may
be less capable of taking advantage of longer contexts, while R1’s reasoning directly benefits from longer
initial prompts. On the other hand, the effect of combined hints on Gemini-2.5-Pro and Claude-3.7-Sonnet
appears relatively small, suggesting they can handle longer context yet lacks the ability to leverage them for
effective reasoning for reproducing code changes.
4.4 Interquartile mean evaluation
We report in Figure 5 the interquartile means (IQM) aggregated across all runs, segmented on hint level,
search scaffold, and model. The IQM metric has been shown to be robust to comparisons with a small sample
size, and in Figure 5, and we report 95%confidence intervals bootstrapped from 3 seeds following Agarwal
et al. (2021). At the hint level, we find agents reach the best aggregate performance when given access to all
three hint levels together. For individual hints, the pseudo-code hint performs the best. At the search scaffold
level, multi-AIDE search outperforms all others. Finally, at the model level, we are surprised to find that
Gemini-2.5-Pro and Claude-3.7-Sonnet attains the lowest performance, close to 0 FSR, lagging behind even
the open-weights R1 model.
4.5 Analysis of search trees
To better understand how each agent spends its search budget, we inspect the proportion of different kinds
of nodes in their search trees: buggy nodes, which crash due to runtime errors; improved nodes, which
successfully improved runtime compared to their parent; and unimproved nodes, which do not improve from
their parent. This breakdown of the search trees is presented in Figure 8. We observe that flat search leads to
a higher total proportion of buggy nodes, indicating that initially-proposed solutions are most often incorrect.
We also notice that R1 agents generate more buggy nodes under AIDE and multi-AIDE—the two variants
with debugging steps—suggesting that R1 may be less capable of fixing its own mistakes compared to o3-mini.
Gemini-2.5-Pro tends to generate fewer buggy nodes compared to the other models, yet it lags behind on the
FSR metric (see Figure 4 and Figure 1), suggesting that Gemini produces more robust code at the cost of
correctly implementing the more efficient solutions described in the hints. Surprisingly, Claude-3.7-Sonnet
generates significantly more buggy nodes than the other three models, with the fraction of buggy nodes
80.00.20.40.60.81.0DeepSeek-R1Flat Tree Forest AIDE Multi-AIDE
0.00.20.40.60.81.0o3-mini
0.00.20.40.60.81.0Gemini-2.5-Pro
1 6 11 160.00.20.40.60.81.0Claude-3.7-Sonnet
1 6 11 16 1 6 11 16
Search Step1 6 11 16 1 6 11 16Buggy No Improvement ImprovedFigure 8Fraction of node types across search trees for each model and search method. Notably, branching (i.e. non-flat)
search is beneficial for reducing the proportion of buggy nodes. Further, a majority of non-buggy steps produce
improved nodes for all branching search methods, with the notable exception of Claude-3.7-Sonnet.
gradually overtaking the fraction of working nodes in the search tree, indicating that Claude-3.7-Sonnet
struggles to improve and debug its previous solutions.
The analysis of node types in the search tree provides insight into the discrepancy on the results of Claude-
3.7-Sonnet between the FSRresults in Figure 4 and the IQM results in Figure 5. While the FSRresults
suggest that, on average, Claude-3.7-Sonnet performs comparably to o3-mini, the IQM plot indicates that
Claude-3.7-Sonnet significantly lags behind o3-mini. This discrepancy can be explained by examining the
distribution of node types in the search tree. Claude-3.7-Sonnet is capable of generating working solutions
that substantially improve the FSR. However, it also produces a considerable number of buggy nodes that
result in runtime errors. These errors negatively impact the overall performance as reflected in the IQM plot,
despite the improvements in the averaged FSR.
4.6 Similarity between agent and human solutions
Agents may output solutions with similar performance to human ones, but may still fail to reproduce the
target code changes. We thus assess code similarity between agent and human solutions by comparing code
embedding distances using the SFR-Embedding-Code 2B model (Liu et al., 2024).
Specifically, we normalize the embedding distance between the agent’s code solution and the target human
solution, i.e. the next record, and divide this distance by the embedding distance between the current and the
next human record. Figure 7 depicts the normalized L2 embedding distance recovered with respect to the
9record speedups and for each type of hint. Here the distance recovered is defined as 1−∥ei+1−e′
i+1∥/∥ei+1−ei∥,
where eiis the embedding for Ri, and e′
iis the embedding for the LLM’s attempt at reproducing it. We
observe a stronger correlation between higher similarity score and FSR for richer hint formats, suggesting
that distances under this embedding space can be a meaningful measure of degree of successful reproduction.
As an alternative measure of code similarity, we made use of R1 as a judge, prompting it to assess what fraction
of the ground-truth code changes between the current and next record were successfully reproduced in the
agent’s solution, on a scale of 0 to 1 with a score of 1 corresponding to a completely correct reimplementation.
Appendix C contains a comparison between these judge-based similarity scores and FSR across all agent
variations. We observe clear positive correlation between higher similarity scores and FSR. We provide sample
outputs from R1 judge in Appendix D.
4.7 Experimenting with additional knowledge
Table 4FSR of R′
12worsens when FlexAttention
docs are inserted in the model’s context.
R′
12 DeepSeek-R1 o3-mini
with docs 0.07 ±0.01 0.06 ±0.01
without docs 0.09±0.01 0.10 ±0.01Certain records are particularly challenging for our baseline
agents, such as R12, which achieves its speedup via FlexAt-
tention (Dong et al., 2024), a PyTorch module that enables
performant implementation of custom attention variants and
was released in August 2024, potentially after the knowledge
cut-off of R1 and o3-mini. To determine whether the agents’
poor performance on R12was due to missing in-weights knowl-
edge of this module, we inserted content from the blog post describing FlexAttention (including usage examples)
as an additional hint to the agent (across all hint levels and agent variations). Table 4 shows this additional
hint actually negatively impacts performance on R12, suggesting that recent models may still struggle to
correctly exploit external knowledge that was not present in their training corpus in more complex tasks.
4.8 Cumulative speedrun experiment
2 3 4
T arget Record Index0.00.20.40.60.81.0FSR
Cumulative Speedup Performance
Figure 9 Cumulative Speedup from initial codebase.In this section, we test the models to see if they can
reproduce the record described in the hint by building on
the codebase they generated when reproducing previous
records. Specifically, each task is formulated as a tuple
of⟨R′
i−1,Ri, ti, m⟩where the agent will be given the
codebase it generated for the previous task R′
i−1and the
hint level mfor reproducing the next record Ri, where
the performance is measured by the FSR metric. This is
a challenging yet realistic extension of the reproducibility
task where the agent seeks to cumulatively improve from
the initial codebase. We evaluate the best-performing
model (o3-mini) with the best search scaffold (multi-
AIDE) from our previous evaluations, with access to all
hint levels (L1 + L2 + L3). Results averaged across
three seeds are presented in Figure 9. The agent recovers approximately 60%of the ground-truth speedup for
R′
2starting from R1. Yet its performance drops significantly afterwards, with R′
3recovering only around 20%
of the speed-up, compared to the 60%of speed-up recovered when starting from the ground-truth R2(see
Figure 6). By only the third record, the agent’s solution R′
4fails to reproduce any speedup compared to R4.
5 Limitations and future directions
Our Automated LLM Speedrunning Benchmark serves as a challenging evaluation of an LLM agent’s ability to
reproduce scientific findings specific to LLM training. However, there remain important limits in its capacity
for assessing an agent’s true capability for scientific reproduction, and each of these limitations point the way
to directions for exciting future research.
Scaling up external knowledge. By design, the various hint levels are succinct and easily fit within the context
of the LLMs we tested. Moreover, these hints were manually defined, with the relevant hint directly provided
10as part of the associated task instance. A more realistic setup would provide the agent with the ability
to use external knowledge via some form of function calling, including the ability to store intermediate
results in various kinds of memory structures, e.g. a short-term scratchpad, long-term database, or neural
module (Hermann et al., 2015; Weston et al., 2014). Accessing a wider and potentially accumulating set of
external information would also test the agent’s ability to manage information whose total size may exceed its
context length (Sarthi et al., 2024).
Memorization or generalization? As many of the ground-truth records in the NanoGPT Speedrun were published
potentially before the cut-off date of the models used in our experiments (and thus, most likely of future
models), there is the possibility that models may have already seen these solutions during training (Gupta
and Pruthi, 2025). We find that neither R1 nor o3-mini accurately reproduce the speedups realized in
the ground-truth records, but explicitly disentangling memorization from generalization may become more
necessary as models begin to saturate the benchmark. More advanced techniques for measuring memorization
in LLMs would allow for a more nuanced evaluation (Carlini et al., 2021; Razeghi et al., 2022; Oren et al.,
2023; Deng et al., 2024).
Semantic diffs. Our experiment analysis focuses on FSR and numeric similarity scores between the LLM’s
solution and the corresponding human solution. Moving beyond a similarity score toward more expressive
natural-language summaries, e.g. via automatically-generated commit messages (Jiang et al., 2017), of the
code diffs between LLM and human solutions would allow for more scalable identification of common mistakes
or new innovations with respect to the human solutions.
From LLM speedrun to ML speedrun. The skills needed for strong performance on the LLM speedrun are
necessary but not sufficient for a reliable research agent. To devise agents that generalize to the future series
of advances in the field of machine learning as a whole, we require more complex tasks for both training
and evaluation. Such tasks may span entire multi-file codebases and entail optimizing for other properties of
models beyond training time, such as held-out task performance or memory footprint; may involve distributed
training considerations beyond a single node; and may require the agent to define its own intermediate success
metrics. Most importantly, our benchmark primarily tests for the ability to reproduce results rather than
the ability to innovate. Should LLM solutions on our benchmark begin to outpace human speedrun records,
we may surely view it as a step towards automated scientific discovery. Ultimately, the real test will be in
whether future agents begin to solve open frontier challenges.
6 Conclusions
We introduced the Automated LLM Speedrunning Benchmark, a challenging evaluation of an LLM agent’s
ability to reproduce existing scientific innovations in LLM training, based on reproducing each successive
record in the community-driven NanoGPT Speedrun. Unlike previous benchmarks for automated scientific
reproducibility, our benchmark enables evaluations of an agent’s ability to reproduce not just a single result,
but each incremental advance across a chain of research innovations. We found that even recent, leading
reasoning models, like R1 and o3-mini, when combined with a state-of-the-art agent scaffold, still struggle
to successfully produce speedrun solutions that match the speedups attained by the corresponding human
solutions. Moreover, this gap between human and agent performance persists even when these strong baseline
agents are provided with detailed explanations describing the exact code changes from the previous speedrun
record. Our results suggest that automated reproducibility may serve as a significant obstacle in realizing
reliable, autonomous research agents with current, leading models, and we expand on the potential societal
impacts of our work in Appendix G. We believe the Automated LLM Speedrunning Benchmark can be an
effective testbed for monitoring this crucial capability in future research agents.
References
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement
learning at the edge of the statistical precipice. Advances in neural information processing systems , 34:29304–29320,
2021.
11Autoscience Institute. Carl technical report, 2025. URL https://www.autoscience.ai/blog/
meet-carl-the-first-ai-system-to-produce-academically-peer-reviewed-research .
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,
Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th
USENIX security symposium (USENIX Security 21) , pages 2633–2650, 2021.
Pablo Samuel Castro, Nenad Tomasev, Ankit Anand, Navodita Sharma, Rishika Mohanta, Aparna Dev, Kuba Perlin,
Siddhant Jain, Kyle Levin, Noémi Éltető, et al. Discovering symbolic cognitive models from human and animal
behavior. bioRxiv, pages 2025–02, 2025.
Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu,
Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander Mądry. Mle-bench: Evaluating machine learning
agents on machine learning engineering, 2025. URL https://arxiv.org/abs/2410.07095 .
Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, and Dianbo Liu. Auto-bench: An
automated benchmark for scientific discovery in llms. arXiv preprint arXiv:2502.15224 , 2025.
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong
Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li,
Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi
Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli
Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,
Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan,
Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin
Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan
Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng
Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen,
R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu,
Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,
T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L.
Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao
Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen,
Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X.
Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi,
Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou,
Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang
Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun
Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,
Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng
Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability
in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .
Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in
modern benchmarks for large language models, 2024. URL https://arxiv.org/abs/2311.09783 .
Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: A programming model for
generating optimized attention kernels, 2024. URL https://arxiv.org/abs/2412.05496 .
Harvey Fineberg, National Academies of Sciences, and Medicine. Reproducibility and replicability in science . National
Academies Press, 2019.
Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner,
Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor
Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: A generalist multi-agent
system for solving complex tasks, 2024. URL https://arxiv.org/abs/2411.04468 .
Paul Gauthier. Aider: Ai pair programming in your terminal. https://github.com/Aider-AI/aider , 2025. URL
https://github.com/Aider-AI/aider . Version 0.82.0.
Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix
Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864 ,
2025.
12Tianyang Gu, Jingjin Wang, Zhihao Zhang, and HaoHong Li. Llms can realize combinatorial creativity: generating
creative ideas via llms for scientific research. arXiv preprint arXiv:2412.14141 , 2024.
Tarun Gupta and Danish Pruthi. All that glitters is not novel: Plagiarism in ai generated research, 2025. URL
https://arxiv.org/abs/2502.16487 .
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement
learning that matters. In Proceedings of the AAAI conference on artificial intelligence , 2018.
Karl Moritz Hermann, Laurent Orseau, Shaodi Wang, Soham Tunyasuvut, Hubert Stanczyk, and Charles Blundell.
Teaching machines to read and comprehend. Advances in neural information processing systems , 28, 2015.
Kexin Huang, Ying Jin, Ryan Li, Michael Y Li, Emmanuel Candès, and Jure Leskovec. Automated hypothesis
validation with agentic sequential falsifications. arXiv preprint arXiv:2502.09858 , 2025.
Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating Language Agents on Machine
Learning Experimentation, April 2024. URL https://arxiv.org/abs/2310.03302 .
Theo Jaffrelot Inizan, Sherry Yang, Aaron Kaplan, Yen-hsu Lin, Jian Yin, Saber Mirzaei, Mona Abdelgaid, Ali H
Alawadhi, KwangHwan Cho, Zhiling Zheng, et al. System of agentic ai for the discovery of metal-organic frameworks.
arXiv preprint arXiv:2504.14110 , 2025.
Intology AI. Zochi technical report, 2025. URL https://github.com/IntologyAI/Zochi/blob/main/Zochi_Technical_
Report.pdf .
Siyuan Jiang, Ameer Armaly, and Collin McMillan. Automatically generating commit messages from diffs using neural
machine translation. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE) ,
pages 135–146. IEEE, 2017.
Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide:
Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138 , 2025.
Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, You Jiacheng, Franz Cesista,
Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline, 2024a. URL
https://github.com/KellerJordan/modded-nanogpt .
Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon:
An optimizer for hidden layers in neural networks, 2024b. URL https://kellerjordan.github.io/posts/muon/ .
Andrej Karpathy. nanogpt, 2023. URL https://github.com/karpathy/nanoGPT .
Ross D King, Jem Rowland, Stephen G Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria Liakata, Magdalena
Markham, Pinar Pir, Larisa N Soldatova, et al. The automation of science. Science, 324(5923):85–89, 2009.
Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan
Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual
web tasks. arXiv preprint arXiv:2401.13649 , 2024.
Pat Langley. Scientific discovery: Computational explorations of the creative processes . MIT press, 1987.
Zijie Lin, Yiqing Shen, Qilin Cai, He Sun, Jinrui Zhou, and Mingjun Xiao. Autop2c: An llm-based agent framework
for code repository generation from multimodal content in academic papers, 2025. URL https://arxiv.org/abs/
2504.20115 .
Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu,
Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982 , 2025a.
Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Codexembed: A
generalist embedding model family for multiligual and multi-task code retrieval, 2024. URL https://arxiv.org/abs/
2411.12644 .
Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, and
Dongzhan Zhou. Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition.
arXiv preprint arXiv:2503.21248 , 2025b.
Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully
automated open-ended scientific discovery, 2024. URL https://arxiv.org/abs/2408.06292 .
13Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for
general ai assistants. In The Twelfth International Conference on Learning Representations , 2023.
Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja,
Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina,
Jakob Foerster, Yoram Bachrach, William Yang Wang, and Roberta Raileanu. Mlgym: A new framework and
benchmark for advancing ai research agents, 2025. URL https://arxiv.org/abs/2502.14499 .
Charles O’Neill, Tirthankar Ghosal, Roberta Răileanu, Mike Walmsley, Thang Bui, Kevin Schawinski, and Ioana Ciucă.
Sparks of science: Hypothesis generation using structured paper data. arXiv preprint arXiv:2504.12976 , 2025.
OpenAI. Openai o3-mini: Pushing the frontier of cost-effective reasoning, January 2025. URL https://openai.com/
index/openai-o3-mini . Accessed: 2025-05-14.
Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. Proving test set
contamination in black box language models, 2023. URL https://arxiv.org/abs/2310.17623 .
Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von
Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In The
Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2024. URL
https://openreview.net/forum?id=n6SCkn2QaG .
Roger D Peng. Reproducible research in computational science. Science, 334(6060):1226–1227, 2011.
Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché Buc,
Emily Fox, and Hugo Larochelle. Improving reproducibility in machine learning research (a report from the neurips
2019 reproducibility program). Journal of machine learning research , 22(164):1–20, 2021.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on
few-shot reasoning. arXiv preprint arXiv:2202.07206 , 2022.
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien
Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries
from program search with large language models. Nature, 625(7995):468–475, 2024.
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. Raptor:
Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning
Representations , 2024.
Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific
papers in machine learning, 2025. URL https://arxiv.org/abs/2504.17192 .
Ishaan Shah, Anthony M Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma,
Anil Thomas, Ashish Tanwer, Darsh J Shah, et al. Practical efficiency of muon for pretraining. arXiv preprint
arXiv:2505.02222 , 2025.
Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. Core-bench: Fostering
the credibility of published research through a computational reproducibility agent benchmark. arXiv preprint
arXiv:2409.11363 , 2024.
Herbert Simon. Machine discovery. Foundations of Science , 1:171–200, 1995.
Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays,
Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench:
Evaluating ai’s ability to replicate ai research, 2025. URL https://arxiv.org/abs/2504.01848 .
Christian Steinruecken, Emma Smith, David Janz, James Lloyd, and Zoubin Ghahramani. The automatic statistician.
Automated machine learning: Methods, systems, challenges , pages 161–173, 2019.
David Waltz and Bruce G Buchanan. Automating science. Science, 324(5923):43–44, 2009.
Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li,
Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff,
Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands:
An open platform for ai software developers as generalist agents, 2024. URL https://arxiv.org/abs/2407.16741 .
14Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916 , 2014.
Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen,
Josh Clymer, Jai Dhyani, et al. Re-bench: Evaluating frontier ai r&d capabilities of language model agents against
human experts. arXiv preprint arXiv:2411.15114 , 2024.
Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, and Yulan He. Scireplicate-bench: Benchmarking llms in
agent-driven algorithmic reproduction from research papers. arXiv preprint arXiv:2504.00255 , 2025.
Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David
Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint
arXiv:2504.08066 , 2025a.
Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David
Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search, 2025b. URL
https://arxiv.org/abs/2504.08066 .
John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press.
SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering, May 2024. URL https://arxiv.
org/abs/2405.15793 .
Weiguo Yin. Exact solution of the frustrated potts model with next-nearest-neighbor interactions in one dimension:
An ai-aided discovery. arXiv preprint arXiv:2503.23758 , 2025.
Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. Assistantbench:
Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711 , 2024.
Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improve-
ment. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis , pages
1592–1604, 2024.
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan
Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint
arXiv:2307.13854 , 2023.
15A Reproducing ground-truth speedruns on our hardware
Figure A.1 compares the training times reported2with the training time of running the same code on our
AWS cluster, where we report the mean and standard deviation of three runs. We can see that the two curves
track closely each other and, as expected, there is no training time decrease for the R6→ R 7transition which
corresponds to the PyTorch upgrade (we are using the upgraded version for R1through R6as we were not
aware which one was the previous PyTorch version).
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
Record Number50000010000001500000200000025000003000000Training Time (ms)
Mean Training Time with Standard Deviation
Reported Training Time
3-Runs Averaged Training Time
Figure A.1 Running the human speedrun records.
B Additional results for reproducing individual records
Figures B.1, B.2, B.5, B.6 depict mean FSR of DeepSeek-R1 and o3-mini agents when aggregating by search
scaffold and hint level. The metrics are reported as 95% confidence intervals bootstrapped from 3 seeds,
with IQM being the interquartile mean and the optimality gap being the difference from the best possible
performance. We used the rliable3library for the evaluation of our runs across multiple search scaffolds and
hint levels.
2https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#world-record-history
3https://github.com/google-research/rliable
160.0 0.2 0.3 0.5
FSRMedian
0.0 0.2 0.3
FSRIQM
0.0 0.2 0.3
FSRMean
0.0 0.3 0.6 0.9
FSROptimality GapHint Level
0 1 2 3 1 + 2 1 + 2 + 3Figure B.1 Aggregate performance of DeepSeek-R1 agents by hint level, reported as 95%confidence intervals, boot-
strapped from 3 seeds. We observe that DeepSeek-R1 agents perform better when instructed with a combination of
pseudocode, text and paper-like hints.
-0.1 0.0 0.1 0.2
FSRMedian
-0.1 0.0 0.1
FSRIQM
-0.1 0.0 0.1 0.2
FSRMean
0.0 0.2 0.5 0.8
FSROptimality GapSearch Method
Flat Tree Forest AIDE MultiAIDE
Figure B.2 Aggregate performance of DeepSeek-R1 agents by search scaffold, reported as 95%confidence intervals,
bootstrapped from 3 seeds. The agent maximizes speedup recovery when using the multi-AIDE scaffold
-0.1 0.0 0.1 0.2
FSRMedian
-0.1 -0.1 0.0 0.1
FSRIQM
-0.1 0.0 0.1 0.2
FSRMean
0.0 0.3 0.6 0.9
FSROptimality GapHint Level
0 1 2 3 1 + 2 1 + 2 + 3
Figure B.3 Aggregate performance of Gemini-2.5-Pro agents by hint level, reported as 95%confidence intervals,
bootstrapped from 3 seeds. We observe that Gemini-2.5-Pro agents perform better when instructed with a combination
of pseudocode, text and paper-like hints.
17-0.1 0.0 0.1 0.2
FSRMedian
-0.1 -0.1 0.0 0.1
FSRIQM
-0.1 0.0 0.1 0.2
FSRMean
0.0 0.3 0.6 0.9
FSROptimality GapSearch Method
Flat Tree Forest AIDE MultiAIDEFigure B.4 Aggregate performance of Gemini-2.5-Pro agents by search scaffold, reported as 95%confidence intervals,
bootstrapped from 3 seeds. The agent maximizes speedup recovery when using the flat scaffold
0.0 0.2 0.4
FSRMedian
0.0 0.2 0.3
FSRIQM
0.0 0.2 0.3 0.5
FSRMean
0.0 0.3 0.6 0.9
FSROptimality GapHint Level
0 1 2 3 1 + 2 1 + 2 + 3
Figure B.5 Aggregate performance of o3-mini agents by hint level, reported as 95%confidence intervals, bootstrapped
from 3 seeds. For o3-mini agents the pseudocode hints yield better results
0.0 0.2 0.3
FSRMedian
-0.1 0.0 0.1 0.2
FSRIQM
0.0 0.2 0.3
FSRMean
0.0 0.2 0.5 0.8
FSROptimality GapSearch Method
Flat Tree Forest AIDE MultiAIDE
FigureB.6 Aggregateperformanceofo3-miniagentsbysearchscaffold, reportedas 95%confidenceintervals, bootstrapped
from 3 seeds. The agent demonstrates its best performance with the multi-AIDE scaffold.
18Figures B.7, B.8, B.9, B.10, B.11 show FSR results for individual records for the flat, tree, forest, AIDE
and multi-AIDE scaffolds, respectively. The agent encounters more difficulty in recovering speedups at later
records, which is expected as minimising training time requires more complex changes later on.
0.0% 50.0% 100.0% 150.0%FSR
91.2
32.8
0.00.20.03.00.00.00.07.021.6
9.70.029.420.952.6
18.4
5.1Hints: 0 Model: DeepSeek-R1
39.6
0.023.0
0.072.7
0.01.831.1
2.70.026.816.718.6
0.0Hints: 1 Model: DeepSeek-R1
47.4
31.7
0.00.00.036.5
0.00.00.08.311.41.30.035.2
21.3
7.215.8
0.0Hints: 2 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
26.3
0.00.028.729.9
0.00.026.8
0.02.20.00.60.99.70.012.088.5
0.0Hints: 3 Model: DeepSeek-R1
98.3
66.4
0.01.20.014.3
0.088.4
0.04.723.4
0.90.030.3
15.822.8
7.83.6Hints: 1 + 2 Model: DeepSeek-R1
66.6
42.7
0.036.848.247.5
0.028.767.0
5.313.7
2.00.019.715.834.9
6.51.5Hints: 1 + 2 + 3 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
101.8
67.2
0.03.80.09.00.04.80.01.811.54.30.013.810.92.415.3
0.0Hints: 0 Model: o3-mini
55.2
37.4
0.013.556.354.9
0.092.0
72.0
3.334.1
1.07.213.412.718.0
3.110.0Hints: 1 Model: o3-mini
32.442.6
0.027.152.565.0
0.043.248.9
0.028.8
0.00.019.912.111.012.2
0.2Hints: 2 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
26.922.0
0.00.058.1
2.30.080.2
31.6
3.214.2
0.018.010.96.815.35.80.9Hints: 3 Model: o3-mini
97.4
28.9
0.027.729.463.6
0.090.0
74.6
2.67.90.70.021.0
7.416.48.15.5Hints: 1 + 2 Model: o3-mini
0.00.00.061.464.7
24.9
0.076.967.6
2.537.2
3.717.215.98.513.214.126.3Hints: 1 + 2 + 3 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
67.366.7
0.00.60.03.40.00.00.02.323.5
11.00.018.720.228.619.6
2.3Hints: 0 Model: Gemini-2.5-Pro
100.6
21.8
0.01.00.028.9
0.025.2
0.02.414.112.8
0.021.322.847.4
25.8
3.1Hints: 1 Model: Gemini-2.5-Pro
101.8
64.4
0.01.60.03.60.00.027.0
3.015.87.60.021.236.026.3
10.70.0Hints: 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number0.0% 50.0% 100.0% 150.0%FSR
33.643.3
0.01.028.9
0.00.00.037.8
2.541.9
9.311.922.219.530.527.6
9.8Hints: 3 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
67.164.9
0.00.40.030.3
0.00.00.03.121.513.12.520.937.750.1
10.33.1Hints: 1 + 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
86.2
21.8
0.01.133.2
2.10.027.0
0.02.944.1
10.60.221.024.044.4
18.8
6.5Hints: 1 + 2 + 3 Model: Gemini-2.5-ProFSR Comparison: Flat
Figure B.7 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the flat search
scaffold.
190.0% 50.0% 100.0% 150.0%FSR
101.9
64.8
0.00.80.01.00.00.00.07.118.612.6
0.023.125.825.1
12.54.3Hints: 0 Model: DeepSeek-R1
97.9
0.00.00.00.047.5
0.074.1
22.3
0.00.04.50.034.148.9
22.6
0.0Hints: 1 Model: DeepSeek-R1
0.00.00.00.00.00.00.025.3
0.03.824.0
0.00.035.0
6.224.518.7
0.0Hints: 2 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
25.921.8
0.028.130.3
1.80.00.029.5
2.26.30.00.010.85.76.07.61.1Hints: 3 Model: DeepSeek-R1
67.1
42.6
0.01.418.042.9
0.077.4
0.05.310.81.70.027.329.529.6
13.62.6Hints: 1 + 2 Model: DeepSeek-R1
67.5
43.5
0.021.674.3
3.00.00.00.05.331.8
6.20.033.324.531.1
9.34.3Hints: 1 + 2 + 3 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
99.3
67.0
0.00.30.01.50.00.10.09.916.45.83.828.3
9.730.1
0.07.9Hints: 0 Model: o3-mini
71.1
55.1
0.064.755.666.7
0.084.974.3
3.526.5
0.94.326.018.419.7
3.44.6Hints: 1 Model: o3-mini
17.041.9
0.00.07.766.6
0.012.374.0
2.828.5
0.00.013.516.0
0.30.00.0Hints: 2 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
33.023.1
0.08.557.1
0.00.048.2
0.04.18.50.011.65.717.3
4.75.10.6Hints: 3 Model: o3-mini
91.7
64.1
0.094.997.6
69.2
0.0141.8
47.2
6.116.4
3.30.022.220.5
6.30.022.7Hints: 1 + 2 Model: o3-mini
50.4
21.7
0.055.772.4
34.325.044.145.1
4.051.0
4.48.719.930.7
15.210.72.0Hints: 1 + 2 + 3 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
63.9
0.00.00.00.00.00.00.00.00.03.70.00.00.00.00.017.2
0.0Hints: 0 Model: Gemini-2.5-Pro
98.3
21.7
0.068.8
34.232.4
0.022.8
0.00.00.04.60.04.10.00.00.03.8Hints: 1 Model: Gemini-2.5-Pro
33.643.2
0.036.756.249.8
0.00.055.0
0.00.00.00.02.918.5
4.528.3
0.0Hints: 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number0.0% 50.0% 100.0% 150.0%FSR
33.565.0
0.00.082.2
1.00.059.2
26.5
0.038.8
0.00.02.49.25.69.40.0Hints: 3 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
33.242.0
0.026.532.824.2
0.00.055.2
0.00.00.00.00.03.20.00.00.0Hints: 1 + 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
32.521.3
0.027.564.8
0.60.00.027.8
0.012.76.00.00.065.5
13.811.71.3Hints: 1 + 2 + 3 Model: Gemini-2.5-ProFSR Comparison: TreeFigure B.8 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the tree search
scaffold.
200.0% 50.0% 100.0% 150.0%FSR
50.564.1
0.01.70.01.50.00.00.08.714.55.10.021.424.023.4
0.04.4Hints: 0 Model: DeepSeek-R1
0.00.00.00.00.088.1
0.00.00.00.00.00.00.00.00.035.8
0.0Hints: 1 Model: DeepSeek-R1
0.00.00.00.00.035.7
0.00.00.07.18.60.00.026.829.2
0.00.05.0Hints: 2 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
27.0
0.00.00.629.0
1.50.024.3
0.02.35.91.18.46.644.2
16.9
0.00.0Hints: 3 Model: DeepSeek-R1
91.8
32.0
0.037.2
0.063.7
0.051.1
0.08.216.95.40.040.541.236.2
7.942.0Hints: 1 + 2 Model: DeepSeek-R1
69.4
43.9
0.034.078.9
31.620.439.734.9
5.160.6
6.90.034.725.124.2
5.30.0Hints: 1 + 2 + 3 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
99.6
64.8
0.01.40.07.80.00.00.06.46.54.50.00.010.30.920.5
0.0Hints: 0 Model: o3-mini
69.0
44.4
0.065.4
37.955.1
0.067.766.5
3.835.6
0.00.019.99.617.910.03.9Hints: 1 Model: o3-mini
29.964.9
0.054.553.167.9
0.084.0
43.8
2.430.2
0.00.015.97.12.47.30.2Hints: 2 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
32.821.2
0.026.257.3
0.00.069.4
0.02.617.4
0.011.211.57.110.47.00.9Hints: 3 Model: o3-mini
67.3
43.8
0.048.4
32.165.8
0.0124.9
56.2
4.08.32.60.020.218.923.7
9.213.3Hints: 1 + 2 Model: o3-mini
50.4
33.2
0.037.873.9
53.1
0.082.8
45.0
7.238.6
11.77.635.123.829.323.718.6Hints: 1 + 2 + 3 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
98.6
28.2
0.00.00.03.70.00.00.00.00.00.00.01.26.85.15.70.7Hints: 0 Model: Gemini-2.5-Pro
65.664.3
0.045.8
33.156.8
0.049.5
27.4
0.02.30.015.1
0.00.010.18.042.7Hints: 1 Model: Gemini-2.5-Pro
97.8
64.0
0.01.455.347.6
0.00.027.7
0.010.30.00.04.86.015.79.60.8Hints: 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number0.0% 50.0% 100.0% 150.0%FSR
99.5
63.6
0.00.187.7
2.90.017.329.7
0.075.3
0.09.73.317.721.412.4
0.0Hints: 3 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
66.363.8
0.049.4
32.330.1
0.063.2
25.1
0.09.636.6
0.11.59.824.217.8
0.0Hints: 1 + 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
65.9
43.1
0.020.497.8
2.022.722.761.0
0.013.310.52.93.543.3
25.7
7.80.0Hints: 1 + 2 + 3 Model: Gemini-2.5-ProFSR Comparison: ForestFigure B.9 FSR results (mean and std over 3 runs) for each record, hint format, and model when using forest search
scaffold.
210.0% 50.0% 100.0% 150.0%FSR
50.4
32.7
0.01.50.04.00.00.00.07.715.47.30.036.636.337.125.6
0.0Hints: 0 Model: DeepSeek-R1
0.00.00.034.149.7
0.00.036.1
0.00.00.00.00.048.5
0.025.6
0.0Hints: 1 Model: DeepSeek-R1
0.032.7
0.00.00.034.2
0.00.00.07.314.3
0.00.026.8
0.00.012.6
0.0Hints: 2 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
0.021.4
0.01.257.4
0.00.083.2
25.4
0.032.7
0.01.315.7
0.09.80.00.0Hints: 3 Model: DeepSeek-R1
98.6
64.7
0.021.022.856.9
0.067.2
0.03.630.7
3.10.020.412.614.59.2Hints: 1 + 2 Model: DeepSeek-R1
98.0
69.4
0.058.949.3
25.2
0.019.325.1
7.672.0
53.2
0.031.226.138.4
9.80.0Hints: 1 + 2 + 3 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
102.3
62.7
0.00.80.03.51.40.00.00.516.57.20.00.00.019.57.90.0Hints: 0 Model: o3-mini
67.2
45.5
0.048.3
18.453.6
0.075.786.9
3.729.0
1.99.312.819.816.211.123.4Hints: 1 Model: o3-mini
35.040.7
0.00.054.161.6
0.014.783.2
2.54.10.20.015.010.86.10.00.0Hints: 2 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
20.2
0.00.00.083.2
0.00.092.7
0.00.00.00.013.4
0.00.00.00.00.0Hints: 3 Model: o3-mini
84.1
64.9
0.040.345.867.4
0.0122.4
101.4
0.05.70.00.0106.2
19.523.6
0.02.5Hints: 1 + 2 Model: o3-mini
100.1
64.4
0.049.465.3
45.855.875.3
31.9
5.815.8
1.212.730.0
16.742.1
21.3Hints: 1 + 2 + 3 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
65.1
21.8
0.00.00.01.30.00.04.20.00.00.00.00.05.40.00.00.0Hints: 0 Model: Gemini-2.5-Pro
66.5
42.5
0.00.00.01.10.071.2
24.6
0.00.00.00.00.00.00.026.8
0.0Hints: 1 Model: Gemini-2.5-Pro
65.864.3
0.025.9
0.02.50.00.00.00.08.00.00.03.10.00.00.00.0Hints: 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number0.0% 50.0% 100.0% 150.0%FSR
115.7
21.7
0.00.229.524.9
0.044.4
26.8
0.00.02.70.00.00.00.00.00.3Hints: 3 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
65.663.6
0.026.231.929.2
0.036.825.9
0.00.00.00.00.00.00.06.679.1Hints: 1 + 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
61.8
42.3
0.051.4
31.0
0.749.5
0.068.6
0.00.00.00.00.00.00.40.00.2Hints: 1 + 2 + 3 Model: Gemini-2.5-ProFSR Comparison: AIDEFigure B.10 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the AIDE
search scaffold.
220.0% 50.0% 100.0% 150.0%FSR
112.1
65.5
0.01.60.05.40.00.00.08.716.25.30.045.635.729.533.2
7.5Hints: 0 Model: DeepSeek-R1
97.3
0.00.00.00.046.0
0.082.4
41.3
0.013.6
0.00.021.6
0.00.00.0Hints: 1 Model: DeepSeek-R1
0.00.00.00.00.035.9
0.00.00.07.124.1
0.00.018.4
0.031.7
16.1
0.0Hints: 2 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
33.9
21.7
0.00.643.9
1.60.017.313.82.46.31.20.012.86.17.93.82.4Hints: 3 Model: DeepSeek-R1
99.2
64.9
0.021.8
0.016.4
0.06.50.06.330.7
6.80.037.929.238.7
17.9
3.5Hints: 1 + 2 Model: DeepSeek-R1
60.5
38.8
0.039.964.661.5
0.033.8
13.15.430.0
2.30.024.316.723.1
4.01.7Hints: 1 + 2 + 3 Model: DeepSeek-R10.0% 50.0% 100.0% 150.0%FSR
99.6
65.6
0.00.80.02.30.00.07.59.213.08.45.116.610.58.37.85.9Hints: 0 Model: o3-mini
51.5
36.6
0.064.171.067.4
0.086.4
57.0
3.929.9
0.05.016.712.912.96.20.5Hints: 1 Model: o3-mini
0.020.6
0.00.052.673.0
0.036.852.4
2.65.99.90.014.730.3
3.75.00.0Hints: 2 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
34.1
21.4
0.00.730.0
0.30.059.8
17.0
2.610.20.111.99.220.617.7
5.50.4Hints: 3 Model: o3-mini
98.3
43.8
0.044.489.6
72.2
0.0113.0
29.8
7.739.7
5.90.033.530.526.319.7
6.9Hints: 1 + 2 Model: o3-mini
50.5
32.7
0.049.3
37.642.238.346.357.7
3.832.0
10.99.622.217.118.710.36.6Hints: 1 + 2 + 3 Model: o3-mini0.0% 50.0% 100.0% 150.0%FSR
65.9
21.4
0.00.00.00.60.00.00.00.08.218.1
0.02.210.317.16.22.5Hints: 0 Model: Gemini-2.5-Pro
64.263.0
0.049.0
0.030.2
0.047.148.8
0.018.610.80.03.218.831.521.3
1.8Hints: 1 Model: Gemini-2.5-Pro
65.8
42.4
0.035.5
0.024.2
0.01.70.00.031.8
7.80.00.018.328.723.3
0.0Hints: 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number0.0% 50.0% 100.0% 150.0%FSR
65.3
42.4
0.00.056.5
1.30.06.20.00.044.2
5.812.8
0.021.427.2
0.00.0Hints: 3 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
65.4
42.9
0.021.732.8
5.50.052.1
24.6
0.039.5
2.00.01.218.829.4
10.92.6Hints: 1 + 2 Model: Gemini-2.5-Pro
1234578910111213141516171819
Record Number
33.064.5
0.050.9
32.7
0.423.846.540.2
0.013.441.0
0.06.812.030.6
18.262.8Hints: 1 + 2 + 3 Model: Gemini-2.5-ProFSR Comparison: MultiAIDEFigure B.11 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the multi-AIDE
search scaffold.
23C Additional results for code similarity judge
Figure C.1 shows the LLM judge scores for each record and search method separately. Some records (e.g.
Record 10) have low reproducibility score across all methods and different types of hints, indicating that they
are inherently challenging for an AI Research agent.
123457891011121314151617181920
Record Number0.00.20.40.60.81.0Reproducibility ScoreMethod: Tree
123457891011121314151617181920
Record Number0.00.20.40.60.81.0Reproducibility ScoreMethod: Forest
12345789101112131415161718
Record Number0.00.20.40.60.81.0Reproducibility ScoreMethod: Flat
12345789101112131415161718
Record Number0.00.20.40.60.81.0Reproducibility ScoreMethod: AIDE
123457891011121314151617181920
Record Number0.00.20.40.60.81.0Reproducibility ScoreMethod: MultiAIDEReproducibility Score vs. Record Number (Grouped by Method, Bars by Model/Level)
Model | Level
o3-mini | Level 1 + 2
o3-mini | Level 1 + 2 + 3
Figure C.1 LLM-as-judge evaluation of reproducibility. The y-axis (Reproducibility Score) measures the fraction of
human expert changes which are reproduced by agent-generated code, where 1 means all human expert’s changes are
reproduced.
Judge Prompt
Below is a baseline implementation of a GPT-2 model, followed by two proposed
changes (see code diffs below) to improve the training speed.
The first change is from an expert human. The second change is from an AI
Assistant, aiming to reproduce the improvement made by the expert human.
Inspect the code diffs carefully and provide an objective evaluation of the
AI Assistant’s solution in terms of its similarity with expert human’s solution.
To derive an objective evaluation, first enumerate all the key changes made by
expert human which can affect training speed, and then analyze all the changes
made by the AI Assistant one by one.
Based on understanding of these code changes, derive a percentage score
(between 0 and 1) to quantify what fraction of the key changes
(which has impact on training speed) made by the expert were correctly
implemented in the AI Assistant’s solution.
Return your final score in a JSON object, with the key "reproducibility_score".
# =============== Baseline Implementation ===========
{human_code}
# =============== Change made by Expert Human ===========
{next_human_code}
# =============== Change made by AI Assistant ===========
{agent_code}
240.0 0.2 0.4 0.6 0.8
Judge Score0.00.20.40.60.81.01.21.4FSRMethods
Tree
ForestAIDE
Multi-AIDEFlatHint Levels
level 1+2 level 1+2+3Figure C.2 How FSR (per record) correlates with LLM judge scores for o3-mini-based agents, where a higher judge
score means the agent solution is closer to the corresponding human speedrun record.
25D Prompts and formatting templates
In this section we present the prompts we use for the coder component (Aider) of our agent scaffold
(Figures D.4, D.5), for the analyzer used by the scaffold to summarize code execution results, i.e. standard
streams, (Figures D.6, D.7) and for drafting initial hints with R1 (Figures D.8, D.9, D.10)
Summary format
Hypothesis: {hypothesis}
Results:
{metrics}
Has bugs? {has_bugs}
Outcome summary:
{outcome_summary}
Figure D.1 Template for rendering results.json , which summarizes each node’s execution and evaluation results.
History format (Example with a single templated version history)
<version_log>
<info>
Version: {version}
Parent version: {parent_version}
Hypothesis: {hypothesis}
Results:
{metrics}
Has bugs? {has_bugs}
Outcome summary:
{outcome_summary}
</info>
...
</version_log>
Figure D.2 Template rendering relevant search history in the coder prompts for debugging and improving nodes.
Knowledge component (Example with two templated entries)
<knowledge>
<li>
{knowldge_entry}
</li>
...
</knowledge>
Figure D.3 Template for the knowledge component of the coder, where each knowledge_entry variable can be an
arbitrary piece of text from an external source.
26Coder Prompt
You are a machine learning scientist, with expertise in large language models
and high-performance computing. Use your expertise to assist the user in their
machine learning task.
Study the current version of {fnames}:
{code}
Your goal is to implement the following ideas to improve the code so that it
better achieves the task:
# Task description
Improve train_gpt2.py so that it achieves or goes below the
target val_loss value of 3.28 in the shortest train_time possible.
Make sure your code changes preserve these aspects of train_gpt2.py:
- The script continues to be runnable via simply calling ‘torchrun
--nproc_per_node=8 train_gpt2.py‘.
- Do NOT change the value of train_files, val_files, or val_token values in
the Hyperparameters config used to set the training args.
- Make sure the values of these hyperparameters are not changed,
and keep to using the current os.environ variables.
- Always keep save_checkpoint set to False in the training args.
- Keep all print0 statements the same. Do not change the arguments
used in the current print0 statements, so to ensure the logging format is
preserved.
- When possible, just change the train_gpt2.py file without making extra files.
- Important: I care about optimizing the performance of the implementation and
do not care how organized or disorganized the code is.
- Any bugs will be described in the "outcome_summary" value of the summary, if
provided. Always focus on addressing these when present, before improving
other parts of the code.
If you violate any of the above constraints, the experiment run will be invalid.
Your job will be run on a single 8xH100 node with access to all 8 GPUs.
You have access to the following knowledge, consider these when writing code:
{knowledge}
**Never** install or ask to install any additional packages. Assume you have
access to the following packages outside of the standard python packages:
{packages}
If necessary, you may access pretrained model checkpoints via HuggingFace for
smaller models like BERT variants or CLIP.
To help with your task, here is a list summarizing recent erroneous changes to
the above code that you have previously tried, along with a summary of the
outcome of each change.
{history}
I trust you to make good decisions, so do not ask me for permission to make any
code changes.
Do not ever ask to install any additional packages. The answer
will be no.
In your final response, include ONLY the fully-functional updated code
which implements ideas in the hypothesis above. Do NOT include any other
content in your final response besides the code.
Figure D.4 Full prompt for the coder (Aider), conditioning on external knowledge. Here, historyandknowledge
template strings are first composed via the templates in Figure D.2 and D.3.
27Coder Prompt With No Knowledge
You are a machine learning scientist, with expertise in large language models
and high-performance computing. Use your expertise to assist the user in their
machine learning task.
Study the current version of {fnames}:
{code}
Your goal is to implement the following ideas to improve the code so that it
better achieves the task:
# Task description
Improve train_gpt2.py so that it achieves or goes below the
target val_loss value of 3.28 in the shortest train_time possible.
Make sure your code changes preserve these aspects of train_gpt2.py:
- The script continues to be runnable via simply calling ‘torchrun
--nproc_per_node=8 train_gpt2.py‘.
- Do NOT change the value of train_files, val_files, or val_token values in
the Hyperparameters config used to set the training args.
- Make sure the values of these hyperparameters are not changed,
and keep to using the current os.environ variables.
- Always keep save_checkpoint set to False in the training args.
- Keep all print0 statements the same. Do not change the arguments
used in the current print0 statements, so to ensure the logging format is
preserved.
- When possible, just change the train_gpt2.py file without making extra files.
- Important: I care about optimizing the performance of the implementation and
do not care how organized or disorganized the code is.
- Any bugs will be described in the "outcome_summary" value of the summary, if
provided. Always focus on addressing these when present, before improving
other parts of the code.
If you violate any of the above constraints, the experiment run will be invalid.
Your job will be run on a single 8xH100 node with access to all 8 GPUs.
**Never** install or ask to install any additional packages. Assume you have
access to the following packages outside of the standard python packages:
{packages}
If necessary, you may access pretrained model checkpoints via HuggingFace for
smaller models like BERT variants or CLIP.
To help with your task, here is a list summarizing recent erroneous changes to
the above code that you have previously tried, along with a summary of the
outcome of each change.
{history}
First, analyze the task and come up with a plan for solving the task:
1. Consider ideas for changes and improvements needed to improve on the task.
Consider both creative and practical ideas.
2. Break down the implementation into clear steps, generate pseudo codes for
each step
3. Consider potential challenges and how to address them
Then, implement your plan by making the necessary code changes.
I trust you to make good decisions, so do not ask me for permission to make
any code changes.
Do not ever ask to install any additional packages. The answer will be no.
Respond with your plan for improving the code, followed by the fully-functional
updated code implementing your plan.
Figure D.5 Full prompt for the coder, without external knowledge. Here, the coder is prompted to first conceive of a
plan for solving the task.
28Log summarization prmopt
Task: Analyze the following output logs and extract metrics following the
metrics structure and typing template provided below.
# Logs
{logs}
# Metric dict template (showing expected type for each key)
{metric_types}
Respond with only the extracted metrics as a JSON dict following the exact
structure and type specification in the dict template below.
If no metrics are successfully extracted, return the empty dict, {{}}. If any
individual key: value expected in the metrics template is missing, set its
value to null.
Figure D.6 Prompt for extracting metrics resulting from executing a solution. Here the logs are a concatenation of the
standard streams output by running the solution.
Standard stream summarization prompt
Task: Produce a succinct summary of the following stdout and stderr logs for a
job running on a compute cluster.
- Your summary should consider whether the logs indicate whether the goal below
was achieved or not.
- Keep your summary below 500 words.
# Job goal
{goal}
# stdout logs
{log_out}
# stderr logs
{log_err}
Respond with just your summary text with no extra commentary and no extra
formatting. If appropriate, include the most useful stderr logs for debugging
in code blocks fenced by triple ticks.
Figure D.7 Prompt for extracting standard stream summaries and metrics resulting from executing a solution.
29Level 1 hint generation prompt
Given the git diff between the current and next version and the changelog,
generate a high-level pseudo code description of the changes made.
Focus on explaining the key algorithmic changes and improvements in a clear,
concise way.
Git diff:
{diff}
Changelog:
{changelog}
Generate pseudo code that:
1. Describes the key algorithmic changes and improvements
2. Focuses on the high-level logic and avoids implementation details
3. Explains the purpose and impact of each major change
4. Uses clear, readable pseudo code syntax
Format the output as:
# Pseudo Code Changes
[Your pseudo code description here]
Figure D.8 Prompt for generating the level 1 (pseudocode)s hints of the Automated LLM Speedrunning benchmark,
where the changelog contains descriptions of the changes retrieved by the repo.
Level 2 hint generation prompt
Given the current code, changelog, and next code, provide a detailed natural
language description of the improvements made.
Current code:
{code}
Changelog:
{changelog}
Next code:
{next_code}
Provide a detailed explanation of:
1. What specific improvements were made
2. Why these changes were beneficial
3. How they contribute to the overall performance
4. Any technical challenges that were addressed
Figure D.9 Prompt for generating the level 2 (text) hints of the Automated LLM Speedrunning benchmark, where the
changelog contains descriptions of the changes retrieved by the repo and next_code is the full implementation of the
next record.
30Level 3 hint generation prompt
Given the current code, changelog, and next code, pseudo codes and text
description, generate a formal paper-like summary of the improvements.
Current code:
{code}
Changelog:
{changelog}
Next code:
{next_code}
Pseudo code:
{generate_level_1(record)}
Text description:
{generate_level_2(record)}
Use this text description and pseudocode changes to generate a body of knowledge
resembling a scientific paper. You should tailor the generated scientific paper
so that a competent machine learning engineer can easily implement the suggested
changes in PyTorch. Besure to include the pseudocode in the paper-like summary.
Figure D.10 Prompt for generating the level 3 hints of the Automated LLM Speedrunning benchmark, where the
changelog contains descriptions of the changes retrieved by the repo and next_code is the full implementation of the
next record.
31E Record breakdown
Table E.1 lists each NanoGPT Speedrun record and its description from the official repository (Jordan et al.,
2024a)4. Each record index is shown with its corresponding task index in Automated LLM Speedrunning,
including its corresponding target next record (indexed by original record index).
Table E.1 Summarized and categorized of records from (Jordan et al., 2024a)
# ID # Transition Record time Description Category
1-- 45 mins llm.c baseline Baseline
21#1→#2 31.4 mins Tuned learning rate & rotary em-
beddingsEmbeddings
32#2→#3 24.9 mins Introduced the Muon optimizer Optimizer
43#3→#4 22.3 mins Muon improvements Optimizer
54#4→#5 15.2 mins Pad embeddings, ReLU ², zero-init
projections, QK-normArchitecture
65#5→#6 13.1 mins Distributed the overhead of Muon Parallelization
7-- 12.0 mins Upgraded PyTorch 2.5.0 Framework
87#6→#8 10.8 mins Untied embedding and head Architecture
98#8→#9 8.2 mins Value and embedding skip connec-
tions, momentum warmup, logit
softcapArchitecture
109#9→#10 7.8 mins Bfloat16 activations Data Type
1110#10→#117.2 mins U-net pattern skip connections &
double lrArchitecture
1211#11→#125.03 mins 1024-ctx dense causal attention →
64K-ctx FlexAttentionAttention Mechanism
1312#12→#134.66 mins Attention window warmup Attention Mechanism
1413#13→#144.41 mins Value Embeddings Embeddings
1514#14→#153.95 mins U-net pattern value embeddings, as-
sorted code optimizationsEmbeddings
1615#15→#163.80 mins Split value embeddings, block slid-
ing window, separate block maskEmbeddings
1716#16→#173.57 mins Sparsify value embeddings, improve
rotary embeddings, drop an atten-
tion layerEmbeddings
1817#17→#183.4 mins Lower logit softcap from 30 to 15 Hyperparameter Tuning
1918#18→#193.142 mins FP8 head, offset logits, lr decay to
0.1 instead of 0.0Data Type
2019#19→#202.992 mins Merged QKV weights, long-short at-
tention, attention scale, lower Adam
epsilon, batched MuonAttention Mechanism
2120#20→#212.933 mins Reduced batch size Hyperparameter Tuning
4https://github.com/KellerJordan/modded-nanogpt
32F Example hints
In this section, we provide example hints used for various hint levels.
Level 1 hint (pseudo-code) for Record 1
# Pseudo Code Changes
1. Rotary Position Embedding Implementation
# Added rotary position embeddings to attention mechanism
class RotaryPositionEmbedding:
def __init__(dim, base=10000):
precompute inverse frequencies using base^(2i/dim)
initialize cache for cos/sin values
def forward(sequence_length):
if sequence_length not in cache:
compute angular positions t
calculate frequency components
store cos(t), sin(t) in cache
return cached cos/sin values
def apply_rotary_embeddings(q, k, cos, sin):
split q and k vectors into halves
rotate components using:
rotated_q = q1*cos + q2*sin
rotated_k = k1*cos + k2*sin
return concatenated rotated vectors
2. Modified Attention Mechanism
class SelfAttention:
def __init__():
# Changed from standard positional embeddings
add rotary embedding module
remove position embedding matrix
def forward(x):
split into q,k,v with same head_dim
apply rotary embeddings to q and k
use scaled_dot_product_attention with rotated q/k
remove manual scaling (was /sqrt(24))
return attention output
3. Layer-Wise Attention Scaling
class TransformerBlock:
def __init__():
# Added depth-dependent scaling
attn_scale = 1/sqrt(2 * num_layers)
def forward(x):
x += attn_scale * attention_output
x += mlp_output
4. Simplified Model Architecture
class GPT:
def __init__():
remove position embedding matrix (wpe)
keep only token embeddings (wte)
remove custom embedding initialization
def forward():
# Position info now handled by rotary embeddings
use only token embeddings (no pos_emb addition)
5. Training Process Improvements
Training Hyperparameters:
batch_size: 32 →64
total_batch_size: 262k →524k tokens
add warmdown phase after constant LR period
Optimization Changes:
replace gradient clipping with:
grad = grad / (norm + 1e-6)
implement linear warmdown schedule
add periodic model checkpoint saving
Learning Rate Schedule:
if step < warmup: linear increase
elif step < total - warmdown: constant
else: linear decrease to zero
Key Impacts:
- Rotary embeddings improve position awareness in attention
- Layer-wise scaling stabilizes deep networks
- Modified LR schedule enables better convergence
- Gradient normalization replaces clipping for stability
- Larger batches improve training efficiency
33Level 2 hint (text description) for Record 1
Here’s a detailed breakdown of the improvements:
1. **Architectural Improvements**
- **Rotary Positional Embeddings**: Replaced standard positional embeddings
with rotary embeddings
- Added ‘Rotary‘ module and ‘apply_rotary_emb‘ function for relative
position encoding
- Benefits: Better captures relative positions and attention patterns,
improves model accuracy
- Implementation: Applied to queries/keys in attention instead of separate
positional embeddings
- **Simplified Normalization**
- Removed all affine parameters from RMSNorm implementation
- Benefits: Reduces parameter count while maintaining effectiveness
- Tradeoff: Minor performance cost offset by other optimizations
2. **Optimization Improvements**
- **Learning Rate Changes**:
- Increased base LR from 0.0015 to 0.0018 (3x increase as per changelog)
- Changed schedule to trapezoidal (warmup →constant→warmdown)
- Benefits: Following [2405.18392], allows more stable high-LR training
- **Gradient Normalization**:
- Replaced gradient clipping with per-parameter gradient norm scaling
- ‘p.grad = p.grad / (p.grad.norm() + 1e-6)‘
- Benefits: More stable training with high LR, prevents explosion
3. **Initialization/Scaling Changes**
- **Attention Scaling**:
- Introduced ‘attn_scale = 1/sqrt(2*n_layer)‘
- Replaced ad-hoc ‘/ math.sqrt(24)‘ with systematic layer-based scaling
- Benefits: Better coordinates residual branches across layers
- **Removed Positional Embeddings**:
- Deleted ‘wpe‘ embedding layer completely
- Benefits: Parameter reduction + rotary handles position information
4. **Training Process Improvements**
- **Checkpointing**:
- Added periodic model saving (‘save_every‘ parameter)
- Benefits: Fault tolerance and easier resumption
- **Batch Size Optimization**:
- Increased batch size from 32 to 64 tokens
- Total batch size from 262K to 524K tokens
- Benefits: Better hardware utilization
5. **Code Simplifications**
- Removed ‘_init_weights‘ and special initialization flags
- Eliminated position embedding mixing (‘tok_emb + pos_emb‘)
- Removed unused configuration options and legacy code paths
**Technical Challenges Addressed**:
1. **Stability at High Learning Rates**: Through gradient normalization and
careful attention scaling
2. **Position Encoding Migration**: Non-trivial transition from absolute to
relative (rotary) positioning
3. **Distributed Training Coordination**: Maintained DDP compatibility through
architectural changes
4. **Learning Rate Schedule Tuning**: Required careful warmup/warmdown
balancing for trapezoidal schedule
**Performance Impact**:
Combined these changes enable 2x faster training convergence by:
34Level 2 hint (text description), continued
- Allowing more aggressive learning rates through better normalization
- Improving parameter efficiency with rotary embeddings
- Increasing useful batch size through stability improvements
- Reducing computational overhead from simplified operations
The architectural simplifications also make the model more amenable to compiler
optimizations (like torch.compile), while the training process improvements
enable better utilization of distributed hardware.
Level 3 hint (mini-paper) for Record 1
# Efficient Training of GPT-style Models Through Architectural and
Optimization Co-design
## Abstract
We present architectural and optimization improvements enabling 2 ×faster
training of GPT-style models while maintaining equivalent final performance.
Through systematic analysis of position encoding, normalization schemes,
and learning rate scheduling, we demonstrate that:
1) Rotary position embeddings (RoPE) improve position-aware attention
computation;
2) A trapezoidal learning rate schedule with gradient normalization enables
more stable high-rate training;
3) Simplified initialization and scaled residual connections reduce parameter
count while maintaining model capacity. Our modifications require minimal code
changes while achieving 5B token convergence equivalent to baseline 10B token
performance.
## 1. Introduction
### 1.1 Background
Transformer architectures (Vaswani et al., 2017) require careful coordination
of position encoding, normalization, and optimization parameters to achieve
efficient training. We analyze common pain points in standard implementations:
- Additive positional embeddings limit attention head flexibility
- Unstable gradient flow requiring aggressive clipping
- Suboptimal learning rate schedules wasting compute
### 1.2 Key Improvements
Our modified architecture (Figure 1) implements four fundamental changes:
1. **Rotary Position Embeddings**: Replace additive positional encoding with
rotational transformations of query/key vectors
2. **Layer-Scaled Attention**: Fixed scaling of attention outputs based on
network depth
3. **Trapezoidal LR Schedule**: Three-phase schedule combining warmup, sustain,
and cooldown periods
4. **Gradient Normalization**: Per-parameter gradient scaling replaces global
clipping
## 2. Methodology
### 2.1 Rotary Position Encoding
Traditional approaches concatenate positional embeddings to token embeddings.
We implement rotary position encoding in attention computation:
‘‘‘python
class Rotary(nn.Module):
def forward(self, x):
t = arange(seq_len)
freqs = outer_product(t, inv_freq)
return cos(freqs), sin(freqs)
35Level 3 hint (mini-paper), continued
def apply_rotary_emb(q, k, cos, sin):
return (q * cos + rotate(q, sin),
k * cos + rotate(k, sin))
‘‘‘
This creates position-aware transformations without additional embedding
parameters. The rotation operation preserves relative position information
through dot product attention.
### 2.2 Trapezoidal Learning Schedule
Our three-phase schedule improves upon cosine decay:
‘‘‘
Learning Rate Schedule:
1. Warmup (0 <= step < 256): lr = base * step/256
2. Sustain (256 <= step < N-2000): lr = base
3. Cooldown (N-2000 <= step <= N): lr = base * (N-step)/2000
‘‘‘
Mathematically:
$$
\text{LR}(t) = \begin{cases}
\alpha\frac{t}{\tau_w} & t \leq \tau_w \\
\alpha & \tau_w < t \leq T-\tau_d \\
\alpha\frac{T-t}{\tau_d} & t > T-\tau_d
\end{cases}
$$
Where $\alpha=0.0018$, $\tau_w=256$, $\tau_d=2000$.
### 2.3 Gradient Normalization
Replaces global gradient clipping with per-parameter scaling:
‘‘‘python
# Before: Global clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
# After: Per-parameter normalization
for p in model.parameters():
p.grad = p.grad / (p.grad.norm() + 1e-6)
‘‘‘
This prevents extreme gradient magnitudes while maintaining relative update
directions.
## 3. Architectural Modifications
### 3.1 Simplified Attention Scaling
Layer-dependent scaling stabilizes deep networks:
‘‘‘python
class Block(nn.Module):
def __init__(self, config):
self.attn_scale = 1/math.sqrt(2*config.n_layer)
def forward(self, x):
x = x + self.attn_scale * attn_output
‘‘‘
residual path accumulation in deep networks.
### 3.2 Parameter Reduction
Removed components:
1. Positional embedding matrix (wpe)
2. Affine parameters in RMSNorm
3. Custom weight initialization
36Level 3 hint (mini-paper), continued
Preserves weight tying between input/output embeddings while reducing total
parameters by 1.2%
## 4. Implementation Details
### 4.1 Critical Code Changes
Core modifications from baseline implementation:
‘‘‘python
# Additions
class Rotary(nn.Module): ...
def apply_rotary_emb(...): ...
# Modifications
class CausalSelfAttention:
def forward():
q, k = apply_rotary_emb(q, k) # Rotate Q/K
class Block:
def __init__():
self.attn_scale = ... # Layer-dependent scaling
# Removals
del self.wpe # Positional embedding matrix
del _init_weights # Custom initialization
‘‘‘
### 4.2 Training Configuration
Updated hyperparameters:
| Parameter | Original | Modified |
|-------------------|----------|----------|
| Batch size | 32 | 64 |
| Total batch tokens| 262k | 524k |
| Peak LR | 0.0015 | 0.0018 |
| LR schedule | Cosine | Trapezoidal |
| Warmup iterations | 256 | 256+2000 |
## 5. Results & Analysis
### 5.1 Performance Improvements
Validation loss comparison on 5B tokens:
| Model | Loss (Original) | Loss (Modified) |
|-------------|-----------------|-----------------|
| d12 (124M) | 3.21 | 3.09 (-3.7%
| d24 (355M) | 2.89 | 2.77 (-4.1%
Achieves equivalent loss to baseline in half the tokens.
### 5.2 Training Dynamics
Key observations:
- 18%
- 2.1×fewer iterations to reach target loss
- 23%
## 6. Conclusion
We demonstrate that coordinated architectural and optimization changes enable
2×faster training of transformer models. Critical factors include:
1. Position encoding through rotary transformations
2. Stable gradient flow via layer-wise scaling
3. Sustained high learning rates through trapezoidal scheduling
37Level 3 hint (mini-paper), continued
The modifications require <200 lines of code changes while maintaining
compatibility with standard distributed training setups. Our results suggest
that careful component co-design remains crucial for efficient large model
training.
## Appendix: Complete Pseudocode
Algorithm 1: Modified Training Loop
‘‘‘python
Initialize model with rotary layers
Load data with doubled batch size
for step in total_steps:
# Forward pass with rotary embeddings
cos, sin = rotary(q)
q = q * cos + rotate(q, sin)
loss = model(x)
# Normalized backward pass
loss.backward()
for p in parameters:
p.grad /= (norm(p.grad) + 1e-6)
# Trapezoidal LR update
lr = trapezoidal_schedule(step)
optimizer.step(lr)
‘‘‘
This paper-style summary provides both theoretical justification and
implementation-level details, enabling direct reproduction of the improvements.
The combination of architectural simplification and optimized training dynamics
demonstrates a practical path for efficient large language model training.
38G Broader impact
AI agents that are proficient in conducting AI research autonomously can provide significant, far-reaching
benefits: (1) accelerated scientific progress in healthcare, climate science, and other important domains, (2)
economic growth driven by the development of novel technology, and (3) expedited safety and alignment
research for models. Crucial to automated science is the ability of such agents to reproduce scientific results,
which our benchmark seeks to measure. However, such innovation also requires a thorough understanding
of model advancements to ensure responsible deployment. We hope our benchmark can serve as a useful
evaluation for model autonomy. However, agents capable of executing open-ended AI research tasks can also
pose risks if their capabilities outpace our ability to comprehend the consequences of their actions. Responsible
deployment of such models therefore requires parallel advancements in monitoring, aligning, and controlling
such models.
To foster understanding, reproducibility, and further development of AI Research Agents, we open-source
the full code to reproduce the experiments on the Automated LLM Speedrunning Benchmark presented in
this work. We acknowledge the limitations of our benchmark and encourage the development of additional
evaluations of automated AI research capabilities, particularly those tailored to the workflow of researchers
training frontier models.
39